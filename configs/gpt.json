{
    "model_args": {
        "activation_function": "gelu_new",
        "attn_pdrop": 0.1,
        "embd_pdrop": 0.1,
        "initializer_range": 0.02,
        "layer_norm_epsilon": 1e-05,
        "n_embd": 768,
        "n_head": 12,
        "n_layer": 12,
        "n_inner": 3072,
        "n_positions": 128,
        "resid_pdrop": 0.1,
        "vocab_size": 32768
    },
    "training_args": {
        "output_dir": "out",
        "dataloader_num_workers": 0,
        "optim": "adamw_torch",
        "learning_rate": 1e-4,
        "num_train_epochs": 30,
        "per_device_train_batch_size": 16,
        "per_device_eval_batch_size": 16,
        "gradient_accumulation_steps": 1,
        "warmup_steps": 200,
        "evaluation_strategy": "epoch",
        "save_strategy": "epoch",
        "weight_decay": 0.01,
        "logging_dir": "logs",
        "logging_steps": 100,
        "overwrite_output_dir": false,
        "save_total_limit": 2,
        "load_best_model_at_end": true,
        "prediction_loss_only": true,
        "bf16": true,
        "torch_compile": false,
        "run_name": "gpt"
    },
    "dataset": "mcgill-babylm/babylm_10M",
    "tokenizer": "mcgill-babylm/tokenizer_bert-base-uncased_32768vocab_10M",
    "group_texts": false,
    "add_pos_tags": false,
    "num_workers": 1,
    "cache_dir": "cache"
}